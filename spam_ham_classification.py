# -*- coding: utf-8 -*-
"""spam/ham classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pqPC3UA0x06e3nizrN2dlEAVUiq-cXSY
"""

import nltk
import pandas as pd
import numpy as np
dataset=pd.read_csv("SMSSpamCollection.tsv",sep="\t",header=None)
dataset.columns=['label','body_text']
dataset.head()

dataset['body_text'][0]

print("input data has {} rows and {} columns".format(len(dataset),len(dataset.columns)))

print("out of {} rows,{} are spam and {} are ham".format(len(dataset),len(dataset[dataset['label']=='spam']),len(dataset[dataset['label']=='ham'])))

print("number of null values in label:{}".format(dataset['label'].isnull().sum()))

print("number of null values in text:{}".format(dataset['body_text'].isnull().sum()))

import string
string.punctuation

def remove_punct(text):
  text_nopunct="".join([char for char in text if char not in string.punctuation ])
  return text_nopunct
dataset['body_text_clean']=dataset['body_text'].apply(lambda x:remove_punct(x))
dataset.head()

import re
def tokenise(text):
    tokens=re.split('\W',text)
    return tokens
dataset['body_text_tokenised']=dataset['body_text_clean'].apply(lambda x:tokenise(x.lower()))
dataset.head()

import nltk
nltk.download('stopwords')

stopwords=nltk.corpus.stopwords.words('english')
def remove_stopwords(tokenised_list):
    text=[word for word in tokenised_list if word not in stopwords]
    return text
dataset['body_text_nostop']=dataset['body_text_tokenised'].apply(lambda x:remove_stopwords(x))
dataset.head()

ps = nltk.PorterStemmer()

def stemming(tokenised_text):
    text = [ps.stem(word) for word in tokenised_text]
    return text

dataset['body_text_stemmed'] = dataset['body_text_nostop'].apply(lambda x: stemming(x))
dataset.head()

import nltk
nltk.download('wordnet')

import nltk
nltk.download('omw-1.4')

wn=nltk.WordNetLemmatizer()
def lemmatising (tokenised_text):
    text=[wn.lemmatize(word) for word in tokenised_text]
    return text
dataset['body_text_lemmatised']=dataset['body_text_nostop'].apply(lambda x: lemmatising(x))
dataset.head()

import string
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('stopwords')

ps = PorterStemmer()

def clean_text(text):
    text = "".join([word.lower() for word in text if word not in string.punctuation])
    tokens = re.split('\W', text)
    text=[ps.stem(word) for word in tokens if word not in stopwords.words('english')]
    return text

count_vect = CountVectorizer(analyzer=clean_text)
X_count = count_vect.fit_transform(dataset['body_text'])
print(X_count.shape)
print(count_vect.get_feature_names_out())

import nltk
from sklearn.feature_extraction.text import CountVectorizer

def tokenize_and_lemmatize(text):
    tokens = nltk.word_tokenize(text)
    lemmatizer = nltk.WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(token) for token in tokens]
    return lemmas

data_sample = dataset[0:20]
count_vect_sample = CountVectorizer(tokenizer=tokenize_and_lemmatize)
X_count_sample = count_vect_sample.fit_transform(data_sample['body_text'])
print(X_count_sample.shape)
print(count_vect_sample.get_feature_names_out())

X_count_sample

x_counts_df=pd.DataFrame(X_count_sample.toarray())
x_counts_df

x_counts_df.columns=count_vect_sample.get_feature_names_out()
x_counts_df

import nltk
from sklearn.feature_extraction.text import TfidfVectorizer

def tokenize_and_lemmatize(text):
    tokens = nltk.word_tokenize(text)
    lemmatizer = nltk.WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(token) for token in tokens]
    return lemmas

tfidf_vect = TfidfVectorizer(tokenizer=tokenize_and_lemmatize)
X_tfidf = tfidf_vect.fit_transform(dataset['body_text'])
print(X_tfidf.shape)
print(tfidf_vect.get_feature_names_out())

import nltk
from sklearn.feature_extraction.text import TfidfVectorizer

def tokenize_and_lemmatize(text):
    tokens = nltk.word_tokenize(text)
    lemmatizer = nltk.WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(token) for token in tokens]
    return lemmas

data_sample = dataset[0:20]
tfidf_vect_sample = TfidfVectorizer(tokenizer=tokenize_and_lemmatize)
X_tfidf_sample = tfidf_vect_sample.fit_transform(data_sample['body_text'])
print(X_tfidf_sample.shape)
print(tfidf_vect_sample.get_feature_names_out())

corpus = ['This is the first document.', 'This is the second second document.', 'And the third one.', 'Is this the first document?']
tfidf_vect_sample = TfidfVectorizer()
X_tfidf_sample = tfidf_vect_sample.fit_transform(corpus)

X_tfidf_df = pd.DataFrame(X_tfidf_sample.toarray())
X_tfidf_df.columns = tfidf_vect_sample.get_feature_names_out()
X_tfidf_df

import nltk
nltk.download('punkt')

import nltk
from sklearn.feature_extraction.text import TfidfVectorizer

def tokenize_and_lemmatize(text):
    tokens = nltk.word_tokenize(text)
    lemmatizer = nltk.WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(token) for token in tokens]
    return lemmas

data_sample = dataset[0:20]
tfidf_vect_sample = TfidfVectorizer(tokenizer=tokenize_and_lemmatize)
X_tfidf_sample = tfidf_vect_sample.fit_transform(data_sample['body_text'])
print(X_tfidf_sample.shape)
print(tfidf_vect_sample.get_feature_names_out())